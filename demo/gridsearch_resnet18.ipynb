{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch for best ResNet model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search over learning rate, batch sizes and layers (using validation loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_val_loss(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=500, patience=15):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss as infinity\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training phase\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss / len(train_loader):.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        scheduler.step(avg_val_loss)  # Learning rate scheduler step based on validation loss\n",
    "\n",
    "        # Check for improvement based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Early stopping check\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs.')\n",
    "            early_stop = True\n",
    "            break\n",
    "    \n",
    "    if not early_stop:\n",
    "        print('Reached maximum epoch limit.')\n",
    "\n",
    "    # Load best model weights based on lowest validation loss\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return best_val_loss  # Return the best validation loss achieved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Example hyperparameters for the grid search\n",
    "batch_sizes = [16, 32]  # Add more as needed\n",
    "learning_rates = [0.001, 0.0005]  # Add more as needed\n",
    "unfreeze_options = ['last', 'last_plus_one']  # Options for layers to unfreeze\n",
    "\n",
    "# Placeholder for best model's performance, now focusing on validation loss\n",
    "best_loss = float('inf')  # Use infinity as the initial value for the best loss\n",
    "best_params = {}\n",
    "\n",
    "# Updated train_and_evaluate function that returns the best validation loss\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for unfreeze_option in unfreeze_options:\n",
    "            print(f\"Training with batch size: {batch_size}, learning rate: {lr}, unfreeze option: {unfreeze_option}\")\n",
    "\n",
    "            # Load a pretrained ResNet model\n",
    "            model = models.resnet18(pretrained=True)\n",
    "            \n",
    "            # Freeze all parameters first\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            # Unfreeze the selected layers based on unfreeze_option\n",
    "            if unfreeze_option == 'last':\n",
    "                for param in model.fc.parameters():\n",
    "                    param.requires_grad = True\n",
    "            elif unfreeze_option == 'last_plus_one':\n",
    "                # Unfreeze both the last block and the fully connected layer\n",
    "                for param in model.layer4.parameters():\n",
    "                    param.requires_grad = True\n",
    "                for param in model.fc.parameters():\n",
    "                    param.requires_grad = True\n",
    "            \n",
    "            model.to(device)\n",
    "\n",
    "            # Assuming you've already defined your datasets\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5, verbose=True)\n",
    "            \n",
    "            # Run training and evaluation, now expecting validation loss as the return value\n",
    "            val_loss = train_and_evaluate_val_loss(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=500, patience=15)  # Adjust num_epochs and patience as needed\n",
    "\n",
    "            # Update best model tracking based on validation loss\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_params = {'batch_size': batch_size, 'learning_rate': lr, 'unfreeze_option': unfreeze_option}\n",
    "\n",
    "print(f\"Best Parameters: {best_params}, Best Validation Loss: {best_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
