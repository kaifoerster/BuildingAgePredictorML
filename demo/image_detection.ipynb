{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age prediction using street view images\n",
    "Link: https://www.kaggle.com/code/gcdatkin/age-prediction-from-images-cnn-regression/notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = Path('/workspace/workspace/ufo-prediction/image_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = pd.Series(list(image_dir.glob('**/*.jpg')), name='Filepath').astype(str)\n",
    "\n",
    "def extract_age_and_name(filepath):\n",
    "    filename = os.path.basename(filepath)  # Get the filename from the filepath\n",
    "    age_and_name = filename.split('.')[0]  # Split by dot and take the first part\n",
    "    age = ''.join(filter(str.isdigit, age_and_name))[:4]  # Extract first 4 digits for age\n",
    "    if age:  # Ensure age string is not empty\n",
    "        age = int(age)\n",
    "    else:  # Default age if no digits found\n",
    "        age = 0\n",
    "    return age\n",
    "\n",
    "ages = pd.Series(filepaths.apply(lambda x: extract_age_and_name(x)), name='Age')\n",
    "\n",
    "# Filter to include only ages above 1900\n",
    "filtered_ages = ages[ages > 1900]\n",
    "\n",
    "# Ensure we only work with filepaths that have a corresponding age above 1900\n",
    "filtered_filepaths = filepaths[ages > 1900]\n",
    "\n",
    "images = pd.concat([filtered_filepaths, filtered_ages], axis=1).sample(frac=1.0, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>873 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Filepath   Age\n",
       "0    /workspace/workspace/ufo-prediction/image_data...  1965\n",
       "1    /workspace/workspace/ufo-prediction/image_data...  1994\n",
       "2    /workspace/workspace/ufo-prediction/image_data...  2009\n",
       "3    /workspace/workspace/ufo-prediction/image_data...  1980\n",
       "4    /workspace/workspace/ufo-prediction/image_data...  2011\n",
       "..                                                 ...   ...\n",
       "868  /workspace/workspace/ufo-prediction/image_data...  1996\n",
       "869  /workspace/workspace/ufo-prediction/image_data...  1953\n",
       "870  /workspace/workspace/ufo-prediction/image_data...  2008\n",
       "871  /workspace/workspace/ufo-prediction/image_data...  2012\n",
       "872  /workspace/workspace/ufo-prediction/image_data...  1950\n",
       "\n",
       "[873 rows x 2 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "train_df, test_df = train_test_split(images, train_size=0.7, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>/workspace/workspace/ufo-prediction/image_data...</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>611 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Filepath   Age\n",
       "502  /workspace/workspace/ufo-prediction/image_data...  1970\n",
       "730  /workspace/workspace/ufo-prediction/image_data...  2003\n",
       "604  /workspace/workspace/ufo-prediction/image_data...  1977\n",
       "246  /workspace/workspace/ufo-prediction/image_data...  1996\n",
       "494  /workspace/workspace/ufo-prediction/image_data...  1991\n",
       "..                                                 ...   ...\n",
       "715  /workspace/workspace/ufo-prediction/image_data...  2004\n",
       "767  /workspace/workspace/ufo-prediction/image_data...  1985\n",
       "72   /workspace/workspace/ufo-prediction/image_data...  2009\n",
       "235  /workspace/workspace/ufo-prediction/image_data...  1975\n",
       "37   /workspace/workspace/ufo-prediction/image_data...  1991\n",
       "\n",
       "[611 rows x 2 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 489 validated image filenames.\n",
      "Found 122 validated image filenames.\n",
      "Found 262 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Age',\n",
    "    target_size=(120, 120),\n",
    "    color_mode='rgb',\n",
    "    class_mode='raw',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Age',\n",
    "    target_size=(120, 120),\n",
    "    color_mode='rgb',\n",
    "    class_mode='raw',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_images = test_generator.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Age',\n",
    "    target_size=(120, 120),\n",
    "    color_mode='rgb',\n",
    "    class_mode='raw',\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 37ms/step - loss: 3904734.7500 - val_loss: 3918913.0000\n",
      "Epoch 2/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 3866138.5000 - val_loss: 3827950.2500\n",
      "Epoch 3/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 3629318.2500 - val_loss: 3320426.7500\n",
      "Epoch 4/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2621991.7500 - val_loss: 1566142.8750\n",
      "Epoch 5/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 631705.5000 - val_loss: 164014.2188\n",
      "Epoch 6/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 190422.5312 - val_loss: 119674.8594\n",
      "Epoch 7/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 131929.8906 - val_loss: 110439.7031\n",
      "Epoch 8/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 115998.1094 - val_loss: 106659.5469\n",
      "Epoch 9/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 112638.1641 - val_loss: 108683.4609\n",
      "Epoch 10/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 112754.6406 - val_loss: 105331.9219\n",
      "Epoch 11/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 109768.8438 - val_loss: 107151.3672\n",
      "Epoch 12/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 110578.0703 - val_loss: 104067.4062\n",
      "Epoch 13/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 109718.5000 - val_loss: 103239.4297\n",
      "Epoch 14/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 110435.0234 - val_loss: 103946.2344\n",
      "Epoch 15/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 107797.6328 - val_loss: 101857.3359\n",
      "Epoch 16/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 106613.0625 - val_loss: 100860.1016\n",
      "Epoch 17/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 105453.2188 - val_loss: 100431.1172\n",
      "Epoch 18/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 104597.8828 - val_loss: 100487.8828\n",
      "Epoch 19/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 103587.5156 - val_loss: 98440.8281\n",
      "Epoch 20/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 103064.8438 - val_loss: 98880.5312\n",
      "Epoch 21/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 102351.9844 - val_loss: 96377.5078\n",
      "Epoch 22/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 101345.1094 - val_loss: 98080.3281\n",
      "Epoch 23/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 101218.2578 - val_loss: 94407.9844\n",
      "Epoch 24/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 99449.6250 - val_loss: 93969.0391\n",
      "Epoch 25/1000\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 98689.0859 - val_loss: 93660.2188\n",
      "Epoch 26/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 97513.1406 - val_loss: 91832.2344\n",
      "Epoch 27/1000\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 95571.1172 - val_loss: 90250.2734\n",
      "Epoch 28/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 94015.0312 - val_loss: 90136.5938\n",
      "Epoch 29/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 93170.5859 - val_loss: 87867.0156\n",
      "Epoch 30/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 92022.6250 - val_loss: 88068.3828\n",
      "Epoch 31/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 91034.9531 - val_loss: 86601.1406\n",
      "Epoch 32/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 89632.2891 - val_loss: 85173.1016\n",
      "Epoch 33/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 87840.6172 - val_loss: 84072.1484\n",
      "Epoch 34/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 87239.5469 - val_loss: 84480.0156\n",
      "Epoch 35/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 84995.7031 - val_loss: 80581.3984\n",
      "Epoch 36/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 85122.3984 - val_loss: 80341.8203\n",
      "Epoch 37/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 82248.6562 - val_loss: 77400.3594\n",
      "Epoch 38/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 80713.0547 - val_loss: 78617.2578\n",
      "Epoch 39/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 79181.4062 - val_loss: 74386.6797\n",
      "Epoch 40/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 78515.5859 - val_loss: 77534.7578\n",
      "Epoch 41/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 76290.1484 - val_loss: 71071.2656\n",
      "Epoch 42/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 73918.0547 - val_loss: 69178.4688\n",
      "Epoch 43/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 72076.3672 - val_loss: 67423.0156\n",
      "Epoch 44/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 70099.2734 - val_loss: 66806.9766\n",
      "Epoch 45/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 67774.5391 - val_loss: 64191.8672\n",
      "Epoch 46/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 65869.5000 - val_loss: 61828.5234\n",
      "Epoch 47/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 63802.8320 - val_loss: 59261.7969\n",
      "Epoch 48/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 63251.6016 - val_loss: 59431.2852\n",
      "Epoch 49/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 58848.9102 - val_loss: 56081.8047\n",
      "Epoch 50/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 56357.2500 - val_loss: 54133.1953\n",
      "Epoch 51/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 53508.8438 - val_loss: 49248.4531\n",
      "Epoch 52/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 50423.6289 - val_loss: 46783.8164\n",
      "Epoch 53/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 47619.2344 - val_loss: 43748.8086\n",
      "Epoch 54/1000\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 45252.4258 - val_loss: 40993.7148\n",
      "Epoch 55/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 41592.6133 - val_loss: 37460.9492\n",
      "Epoch 56/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 37499.7031 - val_loss: 34494.4922\n",
      "Epoch 57/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 34633.3125 - val_loss: 31193.0156\n",
      "Epoch 58/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 31561.4160 - val_loss: 28311.6387\n",
      "Epoch 59/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 28365.3398 - val_loss: 26631.0605\n",
      "Epoch 60/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 24794.8574 - val_loss: 22665.3984\n",
      "Epoch 61/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 22642.9727 - val_loss: 19811.3848\n",
      "Epoch 62/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 19622.3086 - val_loss: 17757.2324\n",
      "Epoch 63/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 17153.8008 - val_loss: 15052.9854\n",
      "Epoch 64/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 15208.1162 - val_loss: 13305.9873\n",
      "Epoch 65/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 12671.4883 - val_loss: 11309.2979\n",
      "Epoch 66/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 10836.8076 - val_loss: 9650.9336\n",
      "Epoch 67/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 9387.2031 - val_loss: 9208.1074\n",
      "Epoch 68/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 8347.0205 - val_loss: 7095.2314\n",
      "Epoch 69/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 6863.5361 - val_loss: 6029.8052\n",
      "Epoch 70/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 6089.1592 - val_loss: 5761.9365\n",
      "Epoch 71/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 5282.6729 - val_loss: 4777.2515\n",
      "Epoch 72/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 4796.2222 - val_loss: 4458.9824\n",
      "Epoch 73/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 4578.2993 - val_loss: 4010.3391\n",
      "Epoch 74/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 4110.7666 - val_loss: 3699.4893\n",
      "Epoch 75/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 3787.9202 - val_loss: 3645.5918\n",
      "Epoch 76/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 3668.1470 - val_loss: 3419.1274\n",
      "Epoch 77/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 3402.1155 - val_loss: 3267.9768\n",
      "Epoch 78/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 3338.3933 - val_loss: 3206.1411\n",
      "Epoch 79/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 3213.9319 - val_loss: 3223.4026\n",
      "Epoch 80/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 3168.4043 - val_loss: 3120.3022\n",
      "Epoch 81/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 3083.9185 - val_loss: 3093.6235\n",
      "Epoch 82/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 3064.5022 - val_loss: 3091.4443\n",
      "Epoch 83/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 3065.6470 - val_loss: 3088.3010\n",
      "Epoch 84/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 3066.9023 - val_loss: 3160.2861\n",
      "Epoch 85/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2988.9062 - val_loss: 2934.1074\n",
      "Epoch 86/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 2961.4089 - val_loss: 2832.3831\n",
      "Epoch 87/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2901.0413 - val_loss: 2924.9041\n",
      "Epoch 88/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2872.7288 - val_loss: 2759.2314\n",
      "Epoch 89/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2866.2119 - val_loss: 2932.6782\n",
      "Epoch 90/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2837.4417 - val_loss: 2785.0215\n",
      "Epoch 91/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2815.0391 - val_loss: 2670.5205\n",
      "Epoch 92/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2796.1545 - val_loss: 2665.8079\n",
      "Epoch 93/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2762.0996 - val_loss: 3160.5811\n",
      "Epoch 94/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2865.8411 - val_loss: 2932.6050\n",
      "Epoch 95/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2883.6797 - val_loss: 2628.7388\n",
      "Epoch 96/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2834.0615 - val_loss: 2722.9514\n",
      "Epoch 97/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 3046.5569 - val_loss: 2777.1179\n",
      "Epoch 98/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2943.1003 - val_loss: 2662.1851\n",
      "Epoch 99/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 3048.7837 - val_loss: 3202.2537\n",
      "Epoch 100/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2665.3901 - val_loss: 2631.1567\n",
      "Epoch 101/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2697.0093 - val_loss: 2478.2446\n",
      "Epoch 102/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2610.8889 - val_loss: 2954.5447\n",
      "Epoch 103/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2743.6194 - val_loss: 2541.1499\n",
      "Epoch 104/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2650.4292 - val_loss: 2436.0459\n",
      "Epoch 105/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2530.8108 - val_loss: 2441.6533\n",
      "Epoch 106/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2554.8877 - val_loss: 2430.4502\n",
      "Epoch 107/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2587.0576 - val_loss: 2414.2126\n",
      "Epoch 108/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2582.9783 - val_loss: 2544.9590\n",
      "Epoch 109/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2553.9055 - val_loss: 2505.9128\n",
      "Epoch 110/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2495.6055 - val_loss: 2332.5508\n",
      "Epoch 111/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2623.0500 - val_loss: 2343.2556\n",
      "Epoch 112/1000\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 2552.4287 - val_loss: 2876.6553\n",
      "Epoch 113/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2539.1672 - val_loss: 2346.2285\n",
      "Epoch 114/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2451.0088 - val_loss: 2282.4309\n",
      "Epoch 115/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2405.6069 - val_loss: 2284.8281\n",
      "Epoch 116/1000\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 2400.1106 - val_loss: 2365.2114\n",
      "Epoch 117/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2427.4565 - val_loss: 2292.0801\n",
      "Epoch 118/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2360.9949 - val_loss: 2243.8022\n",
      "Epoch 119/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2455.6660 - val_loss: 2349.5266\n",
      "Epoch 120/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 2354.4331 - val_loss: 2302.7686\n",
      "Epoch 121/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2343.5212 - val_loss: 2376.5430\n",
      "Epoch 122/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2367.6350 - val_loss: 2193.4985\n",
      "Epoch 123/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2381.9673 - val_loss: 2325.1118\n",
      "Epoch 124/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2380.8643 - val_loss: 2439.2336\n",
      "Epoch 125/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2385.1694 - val_loss: 2129.8447\n",
      "Epoch 126/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2264.3369 - val_loss: 2135.1194\n",
      "Epoch 127/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2314.6619 - val_loss: 2098.2849\n",
      "Epoch 128/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2248.9573 - val_loss: 2189.4587\n",
      "Epoch 129/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2224.3157 - val_loss: 2077.4382\n",
      "Epoch 130/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2301.9392 - val_loss: 2132.1772\n",
      "Epoch 131/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2425.2874 - val_loss: 2137.0532\n",
      "Epoch 132/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2261.5583 - val_loss: 2094.0408\n",
      "Epoch 133/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2347.3337 - val_loss: 2044.2872\n",
      "Epoch 134/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2331.1973 - val_loss: 2069.7429\n",
      "Epoch 135/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2136.9858 - val_loss: 2350.4438\n",
      "Epoch 136/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2301.4790 - val_loss: 1997.8036\n",
      "Epoch 137/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2134.6804 - val_loss: 2089.7593\n",
      "Epoch 138/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2126.8513 - val_loss: 1978.1343\n",
      "Epoch 139/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2117.0291 - val_loss: 2115.9924\n",
      "Epoch 140/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2088.9973 - val_loss: 1970.2749\n",
      "Epoch 141/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2084.6746 - val_loss: 2183.1716\n",
      "Epoch 142/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2236.5322 - val_loss: 1959.3771\n",
      "Epoch 143/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2286.3425 - val_loss: 2135.9443\n",
      "Epoch 144/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2396.8386 - val_loss: 3702.0723\n",
      "Epoch 145/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2346.9475 - val_loss: 1945.1113\n",
      "Epoch 146/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2019.1348 - val_loss: 1901.5931\n",
      "Epoch 147/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2017.6953 - val_loss: 1911.0399\n",
      "Epoch 148/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1997.3995 - val_loss: 1941.1338\n",
      "Epoch 149/1000\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 2056.4038 - val_loss: 1957.6439\n",
      "Epoch 150/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1992.0588 - val_loss: 1959.4615\n",
      "Epoch 151/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1997.9364 - val_loss: 1841.4802\n",
      "Epoch 152/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2043.3348 - val_loss: 1838.3766\n",
      "Epoch 153/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2082.0120 - val_loss: 2049.8115\n",
      "Epoch 154/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1989.7018 - val_loss: 2200.2952\n",
      "Epoch 155/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2090.5217 - val_loss: 1888.4274\n",
      "Epoch 156/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2017.3263 - val_loss: 2284.9539\n",
      "Epoch 157/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2012.7665 - val_loss: 1802.6033\n",
      "Epoch 158/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1965.9821 - val_loss: 1953.1685\n",
      "Epoch 159/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1987.9990 - val_loss: 1840.5133\n",
      "Epoch 160/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1947.1268 - val_loss: 1915.9019\n",
      "Epoch 161/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1865.6788 - val_loss: 1783.0991\n",
      "Epoch 162/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1939.9393 - val_loss: 1914.4552\n",
      "Epoch 163/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1903.7936 - val_loss: 1800.2400\n",
      "Epoch 164/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2050.1816 - val_loss: 1820.0144\n",
      "Epoch 165/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2048.7969 - val_loss: 2010.4244\n",
      "Epoch 166/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1926.0438 - val_loss: 1707.7323\n",
      "Epoch 167/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1941.3054 - val_loss: 2579.3645\n",
      "Epoch 168/1000\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 2042.4266 - val_loss: 1690.2715\n",
      "Epoch 169/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1790.4403 - val_loss: 1685.2180\n",
      "Epoch 170/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1809.8599 - val_loss: 1684.7491\n",
      "Epoch 171/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1823.8448 - val_loss: 1819.4224\n",
      "Epoch 172/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1788.9402 - val_loss: 1945.1229\n",
      "Epoch 173/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1853.5038 - val_loss: 1775.1372\n",
      "Epoch 174/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1747.4679 - val_loss: 1621.6072\n",
      "Epoch 175/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1856.2823 - val_loss: 1651.0526\n",
      "Epoch 176/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1758.6998 - val_loss: 1671.5725\n",
      "Epoch 177/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1782.5763 - val_loss: 2113.5281\n",
      "Epoch 178/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1905.8312 - val_loss: 1691.4019\n",
      "Epoch 179/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1966.3934 - val_loss: 1575.1626\n",
      "Epoch 180/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1954.3208 - val_loss: 2795.6753\n",
      "Epoch 181/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1858.2089 - val_loss: 1575.7106\n",
      "Epoch 182/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1808.2056 - val_loss: 1817.7887\n",
      "Epoch 183/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1697.9628 - val_loss: 1854.7714\n",
      "Epoch 184/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1892.7859 - val_loss: 1639.4966\n",
      "Epoch 185/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1684.8699 - val_loss: 1665.6166\n",
      "Epoch 186/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1681.4547 - val_loss: 1529.3768\n",
      "Epoch 187/1000\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 1832.4824 - val_loss: 2189.3691\n",
      "Epoch 188/1000\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 1744.4061 - val_loss: 1530.1613\n",
      "Epoch 189/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1625.8021 - val_loss: 1942.8866\n",
      "Epoch 190/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1665.9323 - val_loss: 1642.0614\n",
      "Epoch 191/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1843.8839 - val_loss: 1581.7069\n",
      "Epoch 192/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 2020.1713 - val_loss: 2213.7913\n",
      "Epoch 193/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1678.6870 - val_loss: 1492.3389\n",
      "Epoch 194/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1756.6017 - val_loss: 1492.3552\n",
      "Epoch 195/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1634.5255 - val_loss: 1543.4113\n",
      "Epoch 196/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1630.1633 - val_loss: 1443.5957\n",
      "Epoch 197/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1723.8344 - val_loss: 1864.8071\n",
      "Epoch 198/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1622.9617 - val_loss: 1462.3184\n",
      "Epoch 199/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1665.9484 - val_loss: 1418.4573\n",
      "Epoch 200/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1789.2922 - val_loss: 2372.4082\n",
      "Epoch 201/1000\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 1901.0432 - val_loss: 1555.6787\n",
      "Epoch 202/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1710.2512 - val_loss: 1475.9744\n",
      "Epoch 203/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1575.5922 - val_loss: 1400.4113\n",
      "Epoch 204/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1597.1838 - val_loss: 1386.4353\n",
      "Epoch 205/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1582.1898 - val_loss: 1422.9333\n",
      "Epoch 206/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1674.4575 - val_loss: 1540.1641\n",
      "Epoch 207/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1607.7290 - val_loss: 2309.9263\n",
      "Epoch 208/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1614.9838 - val_loss: 1374.1759\n",
      "Epoch 209/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1578.5907 - val_loss: 2147.2764\n",
      "Epoch 210/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1568.4344 - val_loss: 1420.4058\n",
      "Epoch 211/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1671.2395 - val_loss: 1353.6392\n",
      "Epoch 212/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1615.3865 - val_loss: 1857.8716\n",
      "Epoch 213/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1586.6796 - val_loss: 1397.2656\n",
      "Epoch 214/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1496.6639 - val_loss: 1608.3677\n",
      "Epoch 215/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1587.3984 - val_loss: 2144.4805\n",
      "Epoch 216/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1679.4121 - val_loss: 1378.7125\n",
      "Epoch 217/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1560.9404 - val_loss: 1418.0330\n",
      "Epoch 218/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1529.3755 - val_loss: 1313.7029\n",
      "Epoch 219/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1601.8451 - val_loss: 1360.3335\n",
      "Epoch 220/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1551.5079 - val_loss: 1634.3193\n",
      "Epoch 221/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1706.4360 - val_loss: 1311.6051\n",
      "Epoch 222/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1620.4951 - val_loss: 1334.8787\n",
      "Epoch 223/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1649.5481 - val_loss: 1539.3160\n",
      "Epoch 224/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1570.4543 - val_loss: 1285.3792\n",
      "Epoch 225/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1477.5052 - val_loss: 1429.5471\n",
      "Epoch 226/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1452.2347 - val_loss: 1335.4990\n",
      "Epoch 227/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1463.0881 - val_loss: 1358.1716\n",
      "Epoch 228/1000\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 1454.8210 - val_loss: 1278.1439\n",
      "Epoch 229/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1533.2773 - val_loss: 1513.9762\n",
      "Epoch 230/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1487.1315 - val_loss: 1347.4264\n",
      "Epoch 231/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1648.4825 - val_loss: 1548.3807\n",
      "Epoch 232/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1598.7178 - val_loss: 1414.6902\n",
      "Epoch 233/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1556.0332 - val_loss: 1249.8677\n",
      "Epoch 234/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1514.6282 - val_loss: 2170.3889\n",
      "Epoch 235/1000\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 1468.3134 - val_loss: 1322.9978\n",
      "Epoch 236/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1733.0239 - val_loss: 1433.2170\n",
      "Epoch 237/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1536.2491 - val_loss: 1232.3423\n",
      "Epoch 238/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1385.4685 - val_loss: 1240.6304\n",
      "Epoch 239/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1533.0928 - val_loss: 1247.3307\n",
      "Epoch 240/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1474.5240 - val_loss: 1246.0496\n",
      "Epoch 241/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1550.0769 - val_loss: 1252.5901\n",
      "Epoch 242/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1444.3167 - val_loss: 1328.8151\n",
      "Epoch 243/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1459.3311 - val_loss: 1403.4729\n",
      "Epoch 244/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1542.1531 - val_loss: 1347.1310\n",
      "Epoch 245/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1430.0629 - val_loss: 1572.9506\n",
      "Epoch 246/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1456.9203 - val_loss: 1558.5806\n",
      "Epoch 247/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1470.6991 - val_loss: 1239.7749\n",
      "Epoch 248/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1562.2316 - val_loss: 1378.8204\n",
      "Epoch 249/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1587.7953 - val_loss: 1375.7510\n",
      "Epoch 250/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1523.3630 - val_loss: 1189.1310\n",
      "Epoch 251/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1596.8879 - val_loss: 1230.4602\n",
      "Epoch 252/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1738.5847 - val_loss: 1285.1479\n",
      "Epoch 253/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1498.3149 - val_loss: 1260.6600\n",
      "Epoch 254/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1399.9581 - val_loss: 1268.7749\n",
      "Epoch 255/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1414.9484 - val_loss: 1436.7311\n",
      "Epoch 256/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1513.2008 - val_loss: 1318.6659\n",
      "Epoch 257/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1391.6558 - val_loss: 1245.8438\n",
      "Epoch 258/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1395.7552 - val_loss: 1249.8417\n",
      "Epoch 259/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1335.6240 - val_loss: 1161.0480\n",
      "Epoch 260/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1390.1385 - val_loss: 1221.8151\n",
      "Epoch 261/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1413.5447 - val_loss: 2011.8822\n",
      "Epoch 262/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1697.6232 - val_loss: 1161.5964\n",
      "Epoch 263/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1723.4478 - val_loss: 1566.5277\n",
      "Epoch 264/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1719.8706 - val_loss: 1205.0990\n",
      "Epoch 265/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1506.0687 - val_loss: 1153.0374\n",
      "Epoch 266/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1471.3750 - val_loss: 1150.8259\n",
      "Epoch 267/1000\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 1754.8265 - val_loss: 1172.7992\n",
      "Epoch 268/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1623.2159 - val_loss: 1317.1558\n",
      "Epoch 269/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1452.8826 - val_loss: 1678.3403\n",
      "Epoch 270/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1393.0715 - val_loss: 1141.6746\n",
      "Epoch 271/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1366.6482 - val_loss: 1154.0105\n",
      "Epoch 272/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1326.2505 - val_loss: 1139.2876\n",
      "Epoch 273/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1311.5010 - val_loss: 1213.8521\n",
      "Epoch 274/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1414.4401 - val_loss: 1123.7798\n",
      "Epoch 275/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1403.7517 - val_loss: 2239.0515\n",
      "Epoch 276/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1488.7465 - val_loss: 1554.7664\n",
      "Epoch 277/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1479.8645 - val_loss: 1384.0553\n",
      "Epoch 278/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1433.2722 - val_loss: 1119.8102\n",
      "Epoch 279/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1321.1810 - val_loss: 1267.7668\n",
      "Epoch 280/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1429.4030 - val_loss: 1268.1693\n",
      "Epoch 281/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1526.0076 - val_loss: 1152.2429\n",
      "Epoch 282/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1348.0488 - val_loss: 1123.0553\n",
      "Epoch 283/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1520.3633 - val_loss: 1156.3839\n",
      "Epoch 284/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1299.2952 - val_loss: 1103.7250\n",
      "Epoch 285/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1354.5232 - val_loss: 1368.1007\n",
      "Epoch 286/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1697.2852 - val_loss: 1629.4744\n",
      "Epoch 287/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1863.3273 - val_loss: 1228.0839\n",
      "Epoch 288/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1628.7266 - val_loss: 1097.8953\n",
      "Epoch 289/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1318.8116 - val_loss: 1213.7568\n",
      "Epoch 290/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1391.8365 - val_loss: 1632.1664\n",
      "Epoch 291/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1443.6600 - val_loss: 1105.2101\n",
      "Epoch 292/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1288.7705 - val_loss: 1092.4807\n",
      "Epoch 293/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1252.1949 - val_loss: 1182.2687\n",
      "Epoch 294/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1358.4315 - val_loss: 1111.6223\n",
      "Epoch 295/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1509.0754 - val_loss: 1147.2200\n",
      "Epoch 296/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1356.4529 - val_loss: 1122.8712\n",
      "Epoch 297/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1268.2235 - val_loss: 1080.9442\n",
      "Epoch 298/1000\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 1289.1462 - val_loss: 1184.0365\n",
      "Epoch 299/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1513.9147 - val_loss: 1115.4207\n",
      "Epoch 300/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1304.3575 - val_loss: 1084.6117\n",
      "Epoch 301/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1275.7737 - val_loss: 1075.0825\n",
      "Epoch 302/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1271.7611 - val_loss: 1393.9003\n",
      "Epoch 303/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1438.7957 - val_loss: 2006.7720\n",
      "Epoch 304/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1507.8427 - val_loss: 1346.2762\n",
      "Epoch 305/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1412.7997 - val_loss: 1859.3832\n",
      "Epoch 306/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1402.1670 - val_loss: 1245.7859\n",
      "Epoch 307/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1310.1564 - val_loss: 1060.3618\n",
      "Epoch 308/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1417.6111 - val_loss: 1067.8009\n",
      "Epoch 309/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1510.3636 - val_loss: 1294.2418\n",
      "Epoch 310/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1387.0991 - val_loss: 1062.5691\n",
      "Epoch 311/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1349.8412 - val_loss: 1149.2380\n",
      "Epoch 312/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1556.1659 - val_loss: 1156.0085\n",
      "Epoch 313/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1315.7395 - val_loss: 1105.7269\n",
      "Epoch 314/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1375.0460 - val_loss: 1374.2705\n",
      "Epoch 315/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1362.1431 - val_loss: 1059.3260\n",
      "Epoch 316/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1552.8726 - val_loss: 1068.0250\n",
      "Epoch 317/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1276.2789 - val_loss: 1058.3264\n",
      "Epoch 318/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1337.2229 - val_loss: 1065.2255\n",
      "Epoch 319/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1443.7944 - val_loss: 1128.5798\n",
      "Epoch 320/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1346.8977 - val_loss: 1169.7599\n",
      "Epoch 321/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1360.7051 - val_loss: 1266.2324\n",
      "Epoch 322/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1311.5518 - val_loss: 1093.7544\n",
      "Epoch 323/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1270.8182 - val_loss: 1039.6416\n",
      "Epoch 324/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1292.8149 - val_loss: 1187.8885\n",
      "Epoch 325/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1234.3816 - val_loss: 2074.0195\n",
      "Epoch 326/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1351.8467 - val_loss: 1134.8729\n",
      "Epoch 327/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1342.8209 - val_loss: 1830.8132\n",
      "Epoch 328/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1382.0773 - val_loss: 1497.3304\n",
      "Epoch 329/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1274.4618 - val_loss: 1045.2432\n",
      "Epoch 330/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1310.4640 - val_loss: 1046.4651\n",
      "Epoch 331/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1409.2981 - val_loss: 1167.5576\n",
      "Epoch 332/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1509.0989 - val_loss: 1065.1744\n",
      "Epoch 333/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1464.9014 - val_loss: 1045.6799\n",
      "Epoch 334/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1279.0381 - val_loss: 1036.7268\n",
      "Epoch 335/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1274.8760 - val_loss: 1208.0305\n",
      "Epoch 336/1000\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 1458.5056 - val_loss: 1168.4331\n",
      "Epoch 337/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1431.0835 - val_loss: 1132.7666\n",
      "Epoch 338/1000\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 1441.3014 - val_loss: 1377.8402\n",
      "Epoch 339/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1343.5800 - val_loss: 1061.9019\n",
      "Epoch 340/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1265.3032 - val_loss: 1040.8735\n",
      "Epoch 341/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1520.4141 - val_loss: 2713.2449\n",
      "Epoch 342/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1810.7684 - val_loss: 1823.3608\n",
      "Epoch 343/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1269.3599 - val_loss: 1120.2694\n",
      "Epoch 344/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1240.8589 - val_loss: 1035.5044\n",
      "Epoch 345/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1358.2916 - val_loss: 1404.9933\n",
      "Epoch 346/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1455.4342 - val_loss: 1037.7224\n",
      "Epoch 347/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1457.9417 - val_loss: 1090.3594\n",
      "Epoch 348/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1416.5474 - val_loss: 1190.8877\n",
      "Epoch 349/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1304.9042 - val_loss: 1013.5612\n",
      "Epoch 350/1000\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 1335.0704 - val_loss: 1271.7577\n",
      "Epoch 351/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1353.7247 - val_loss: 1047.1898\n",
      "Epoch 352/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1568.4286 - val_loss: 1453.8506\n",
      "Epoch 353/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1464.3147 - val_loss: 1278.4254\n",
      "Epoch 354/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1447.1431 - val_loss: 1060.2062\n",
      "Epoch 355/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1284.0713 - val_loss: 1087.5016\n",
      "Epoch 356/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1181.8597 - val_loss: 1048.0707\n",
      "Epoch 357/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1200.5311 - val_loss: 1006.6131\n",
      "Epoch 358/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1266.3707 - val_loss: 1473.3125\n",
      "Epoch 359/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1323.2356 - val_loss: 1100.6372\n",
      "Epoch 360/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1422.7255 - val_loss: 1001.4829\n",
      "Epoch 361/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1535.0125 - val_loss: 997.1351\n",
      "Epoch 362/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1278.3607 - val_loss: 1077.4944\n",
      "Epoch 363/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1547.0475 - val_loss: 1026.1008\n",
      "Epoch 364/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1348.5073 - val_loss: 1159.7805\n",
      "Epoch 365/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1243.4250 - val_loss: 1011.4332\n",
      "Epoch 366/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1291.2959 - val_loss: 1052.6747\n",
      "Epoch 367/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1283.4215 - val_loss: 1155.5868\n",
      "Epoch 368/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1307.2993 - val_loss: 1593.4207\n",
      "Epoch 369/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1611.7661 - val_loss: 1323.0864\n",
      "Epoch 370/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1549.3901 - val_loss: 2178.6948\n",
      "Epoch 371/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1641.7355 - val_loss: 1542.0548\n",
      "Epoch 372/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1540.6854 - val_loss: 1002.6328\n",
      "Epoch 373/1000\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1847.1676 - val_loss: 986.7972\n",
      "Epoch 374/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1383.0408 - val_loss: 1075.4960\n",
      "Epoch 375/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1180.4573 - val_loss: 989.7212\n",
      "Epoch 376/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1223.6055 - val_loss: 1318.0771\n",
      "Epoch 377/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1194.0012 - val_loss: 1055.1147\n",
      "Epoch 378/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1255.8765 - val_loss: 984.4102\n",
      "Epoch 379/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1236.8719 - val_loss: 992.4737\n",
      "Epoch 380/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1470.9222 - val_loss: 1073.1045\n",
      "Epoch 381/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1390.7944 - val_loss: 1107.7122\n",
      "Epoch 382/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1362.1843 - val_loss: 1555.2659\n",
      "Epoch 383/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1302.5424 - val_loss: 1314.8870\n",
      "Epoch 384/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1400.5504 - val_loss: 1072.0994\n",
      "Epoch 385/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1247.0608 - val_loss: 1096.1931\n",
      "Epoch 386/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1195.8838 - val_loss: 979.2982\n",
      "Epoch 387/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1191.5613 - val_loss: 982.1727\n",
      "Epoch 388/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1323.5414 - val_loss: 1146.3861\n",
      "Epoch 389/1000\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 1269.1385 - val_loss: 1073.5226\n",
      "Epoch 390/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1360.6730 - val_loss: 1948.2815\n",
      "Epoch 391/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1230.0128 - val_loss: 1015.5614\n",
      "Epoch 392/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1180.9147 - val_loss: 1513.4641\n",
      "Epoch 393/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1466.0610 - val_loss: 2376.0012\n",
      "Epoch 394/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1375.9363 - val_loss: 1145.3905\n",
      "Epoch 395/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1179.6913 - val_loss: 1140.6791\n",
      "Epoch 396/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1196.6515 - val_loss: 1274.1616\n",
      "Epoch 397/1000\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1292.0555 - val_loss: 1272.3582\n",
      "Epoch 398/1000\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1199.3173 - val_loss: 1278.2501\n",
      "Epoch 399/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1399.7177 - val_loss: 1149.8571\n",
      "Epoch 400/1000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1164.2223 - val_loss: 1162.6335\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(120, 120, 3))\n",
    "x = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPool2D()(x)\n",
    "x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPool2D()(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    validation_data=val_images,\n",
    "    epochs=1000,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=14,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 17ms/step\n",
      "     Test RMSE: 34.16974\n",
      "Test R^2 Score: -0.19040\n"
     ]
    }
   ],
   "source": [
    "predicted_ages = np.squeeze(model.predict(test_images))\n",
    "true_ages = test_images.labels\n",
    "\n",
    "rmse = np.sqrt(model.evaluate(test_images, verbose=0))\n",
    "print(\"     Test RMSE: {:.5f}\".format(rmse))\n",
    "\n",
    "r2 = r2_score(true_ages, predicted_ages)\n",
    "print(\"Test R^2 Score: {:.5f}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null/Baseline Model Test RMSE: 31.31807\n"
     ]
    }
   ],
   "source": [
    "null_rmse = np.sqrt(np.sum((true_ages - np.mean(true_ages))**2) / len(true_ages))\n",
    "print(\"Null/Baseline Model Test RMSE: {:.5f}\".format(null_rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ufo-predict2 (Tensorflow 2.11.0)",
   "language": "python",
   "name": "kai-ufo-predict2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
